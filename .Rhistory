for(d in 1:D){
for(j in 1:S){
tempM[d, d] = tempM[d, d] + (w[i, j] * ((x[i,d] - x[j,d])^2) + p)
}
}
M[[i]] = solve(tempM)
}
M
a
a[1,1] = 2
a[2,2] = 2
a
solve(a)
solve(2)
library('MASS')
x = as.matrix(iris[,c(1,3)]) #150 x 2
mu = mean(x) # Média estimada
cov = cov(x) # Matriz de covariância estimada
t = 0
S = dim(x)[1] # Numero de observações
D = dim(x)[2] # Numero of atributos
Z = sqrt(((2*pi)^D) * det(cov)) # Constante de normalização da distribuição normal Euclidiana
# Equação 7
sigma = 0.5
w = matrix(0, S, S)
for(i in 1:S){
for(j in 1:S){
w[i, j] = kernel(x[i,], x[j,], sigma)
}
}
d
a
cov
-cov
ginv(cov)
solve(cov)
n
v
v = mvrnorm(S, rep(0, D), cov) # Normal com média 0 e matriz de covariância estimada
v
2 / 2 * 2
2 / (2 * 2)
bigint <- integer(2^32 / 4)
bigint
bigint
pacman::p_load('h2o')
h2o.init(nthreads=-1)
library('ROSE')
library('rpart')
library('xgboost')
library('caret')
path = 'C:/Projects/case_celpe/celpe_case'
setwd(path)
source('utils.r')
# Decision Tree
dtTest = function(dataset){
dt_model = rpart(dataset$trainLabels ~ ., data = dataset$trainNoLabels)
pred = predict(dt_model, newdata = dataset$testNoLabels)
pred_hard = ifelse(pred < 0.50, 0, 1)
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Balanced Deep Learning
deepTest = function(dataset){
# Transformando alvo em fator, para o modelo entender que a variável é binária e não numérica
dataset$train[, 'TARGET'] = as.factor(dataset$train[, 'TARGET'])
dataset$test[, 'TARGET'] = as.factor(dataset$test[, 'TARGET'])
dl_model = h2o.deeplearning(y='TARGET', training_frame=as.h2o(dataset$train), validation_frame=as.h2o(dataset$test),
hidden=c(100, 100), epochs=1, activation='Tanh',
score_training_samples=0,
score_validation_samples=0,
balance_classes=TRUE,
quiet_mode = TRUE)
pred = h2o.predict(dl_model, as.h2o(dataset$test))
pred = as.data.frame(pred)
pred_hard = pred[,1]
pred_prob = pred[,3]
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred_prob, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Extreme Gradient Boosting
xgboostTest = function(dataset){
# Alvo precisa ser 0 e 1 e numérico
trainLabels = as.numeric(dataset$trainLabels)
trainLabels[which(trainLabels == 2)] = 0
bst= xgboost(data = data.matrix(dataset$trainNoLabels, rownames.force = NA),
label = trainLabels, nrounds = 50, objective = "binary:logistic", verbose = 0)
pred = predict(bst, data.matrix(dataset$testNoLabels, rownames.force = NA))
pred_hard = ifelse(pred < 0.50, 0, 1)
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Lendo a base pré processada
dataset = read.table('processed_train.csv', header=T, sep=';')
# Dividindo a base em k folds para validação cruzada
numOfFolds = 10
dataset = splitDataset(dataset, numOfFolds)
aucResults = list()
for(i in 1:numOfFolds){
aucResults$dt = c(aucResults$dt, dtTest(dataset[[i]])[[1]])
aucResults$boost = c(aucResults$boost, xgboostTest(dataset[[i]])[[1]])
aucResults$deep = c(aucResults$deep, deepTest(dataset[[i]])[[1]])
}
test0 = friedman.test(cbind( aucResults$dt, aucResults$boost, aucResults$deep))
test0
test1 = wilcox.test(aucResults$dt, aucResults$boost, paired = TRUE, alternative = "less", conf.level = 0.95)
test1
test2 = wilcox.test(aucResults$dt, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
test2
aucResults$boost
aucResults$dt
test3 = wilcox.test(aucResults$boost, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
test3
wilcox.test(aucResults$dt, aucResults$boost, paired = TRUE, alternative = "less", conf.level = 0.95)
wilcox.test(aucResults$boost, aucResults$dt, paired = TRUE, alternative = "less", conf.level = 0.95)
wilcox.test(aucResults$boost, aucResults$dt, paired = TRUE, alternative = "less", conf.level = 0.95)
wilcox.test(aucResults$boost, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
wilcox.test(aucResults$dt, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
summary(aucResults$boost)
summary(aucResults$dt)
summary(aucResults$deep)
aucResults$deep
pacman::p_load('h2o')
h2o.init(nthreads=-1)
library('ROSE')
library('rpart')
library('xgboost')
library('caret')
path = 'C:/Projects/case_celpe/celpe_case'
setwd(path)
source('utils.r')
# Decision Tree
dtTest = function(dataset){
dt_model = rpart(dataset$trainLabels ~ ., data = dataset$trainNoLabels)
pred = predict(dt_model, newdata = dataset$testNoLabels)
pred_hard = ifelse(pred < 0.50, 0, 1)
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Balanced Deep Learning
deepTest = function(dataset){
# Transformando alvo em fator, para o modelo entender que a variável é binária e não numérica
dataset$train[, 'TARGET'] = as.factor(dataset$train[, 'TARGET'])
dataset$test[, 'TARGET'] = as.factor(dataset$test[, 'TARGET'])
dl_model = h2o.deeplearning(y='TARGET', training_frame=as.h2o(dataset$train), validation_frame=as.h2o(dataset$test),
hidden=c(100, 100), epochs=10, activation='Tanh',
score_training_samples=0,
score_validation_samples=0,
balance_classes=TRUE,
quiet_mode = TRUE)
pred = h2o.predict(dl_model, as.h2o(dataset$test))
pred = as.data.frame(pred)
pred_hard = pred[,1]
pred_prob = pred[,3]
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred_prob, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Extreme Gradient Boosting
xgboostTest = function(dataset){
# Alvo precisa ser 0 e 1 e numérico
trainLabels = as.numeric(dataset$trainLabels)
trainLabels[which(trainLabels == 2)] = 0
bst= xgboost(data = data.matrix(dataset$trainNoLabels, rownames.force = NA),
label = trainLabels, nrounds = 50, objective = "binary:logistic", verbose = 0)
pred = predict(bst, data.matrix(dataset$testNoLabels, rownames.force = NA))
pred_hard = ifelse(pred < 0.50, 0, 1)
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Lendo a base pré processada
dataset = read.table('processed_train.csv', header=T, sep=';')
# Dividindo a base em k folds para validação cruzada
numOfFolds = 30
dataset = splitDataset(dataset, numOfFolds)
# Guardando resultados da área sob a curva ROC
aucResults = list()
for(i in 1:numOfFolds){
aucResults$dt = c(aucResults$dt, dtTest(dataset[[i]])[[1]])
aucResults$boost = c(aucResults$boost, xgboostTest(dataset[[i]])[[1]])
aucResults$deep = c(aucResults$deep, deepTest(dataset[[i]])[[1]])
}
# Analisando os resultados
summary(aucResults$boost)
summary(aucResults$dt)
summary(aucResults$deep)
# Aplicando testes de hipótese para garantir que os resultados diferem
test0 = friedman.test(cbind( aucResults$dt, aucResults$boost, aucResults$deep))
test1 = wilcox.test(aucResults$boost, aucResults$dt, paired = TRUE, alternative = "less", conf.level = 0.95)
test2 = wilcox.test(aucResults$boost, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
test3 = wilcox.test(aucResults$dt, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
saveRDS(aucResults, "aucResults.rds")
a = readRDS("aucResults.rds")
a
test0 = friedman.test(cbind( aucResults$dt, aucResults$boost, aucResults$deep))
test0
# Balanced Deep Learning
deepTest = function(dataset){
# Transformando alvo em fator, para o modelo entender que a variável é binária e não numérica
dataset$train[, 'TARGET'] = as.factor(dataset$train[, 'TARGET'])
dataset$test[, 'TARGET'] = as.factor(dataset$test[, 'TARGET'])
dl_model = h2o.deeplearning(y='TARGET', training_frame=as.h2o(dataset$train), validation_frame=as.h2o(dataset$test),
hidden=c(100, 100), epochs=10, activation='Tanh',
score_training_samples=0,
score_validation_samples=0,
balance_classes=FALSE,
quiet_mode = TRUE)
pred = h2o.predict(dl_model, as.h2o(dataset$test))
pred = as.data.frame(pred)
pred_hard = pred[,1]
pred_prob = pred[,3]
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred_prob, plotit = F)$auc
return (list(auc, cfMatrix))
}
i = 1
deep = deepTest(dataset[[i]])
deep
aucResults = list()
for(i in 1:numOfFolds){
dt = dtTest(dataset[[i]])
aucResults$dt = c(aucResults$dt, dt[[1]])
boost = xgboostTest(dataset[[i]])
aucResults$boost = c(aucResults$boost, boost[[1]])
deep = deepTest(dataset[[i]])
aucResults$deep = c(aucResults$deep, deep[[1]])
}
i = 1
deepTest(dataset[[i]], true)
# Balanced Deep Learning
deepTest = function(dataset, balanced){
# Transformando alvo em fator, para o modelo entender que a variável é binária e não numérica
dataset$train[, 'TARGET'] = as.factor(dataset$train[, 'TARGET'])
dataset$test[, 'TARGET'] = as.factor(dataset$test[, 'TARGET'])
dl_model = h2o.deeplearning(y='TARGET', training_frame=as.h2o(dataset$train), validation_frame=as.h2o(dataset$test),
hidden=c(50, 50), epochs=1, activation='Tanh',
score_training_samples=0,
score_validation_samples=0,
balance_classes=balanced,
quiet_mode = TRUE)
pred = h2o.predict(dl_model, as.h2o(dataset$test))
pred = as.data.frame(pred)
pred_hard = pred[,1]
pred_prob = pred[,3]
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred_prob, plotit = F)$auc
return (list(auc, cfMatrix))
}
deepTest(dataset[[i]], TRUE)[[1]]
deepTest(dataset[[i]], FALSE)[[1]]
deepTest(dataset[[i]], TRUE)[[1]]
deepTest(dataset[[i]], FALSE)[[1]]
pacman::p_load('h2o')
h2o.init(nthreads=-1)
library('ROSE')
library('rpart')
library('xgboost')
library('caret')
path = 'C:/Projects/case_celpe/celpe_case'
setwd(path)
source('utils.r')
# Decision Tree
dtTest = function(dataset){
dt_model = rpart(dataset$trainLabels ~ ., data = dataset$trainNoLabels)
pred = predict(dt_model, newdata = dataset$testNoLabels)
pred_hard = ifelse(pred < 0.50, 0, 1)
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Balanced Deep Learning
deepTest = function(dataset, balanced){
# Transformando alvo em fator, para o modelo entender que a variável é binária e não numérica
dataset$train[, 'TARGET'] = as.factor(dataset$train[, 'TARGET'])
dataset$test[, 'TARGET'] = as.factor(dataset$test[, 'TARGET'])
dl_model = h2o.deeplearning(y='TARGET', training_frame=as.h2o(dataset$train), validation_frame=as.h2o(dataset$test),
hidden=c(50, 50), epochs=1, activation='Tanh',
score_training_samples=0,
score_validation_samples=0,
balance_classes=balanced,
quiet_mode = TRUE)
pred = h2o.predict(dl_model, as.h2o(dataset$test))
pred = as.data.frame(pred)
pred_hard = pred[,1]
pred_prob = pred[,3]
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred_prob, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Extreme Gradient Boosting
xgboostTest = function(dataset){
# Alvo precisa ser 0 e 1 e numérico
trainLabels = as.numeric(dataset$trainLabels)
trainLabels[which(trainLabels == 2)] = 0
bst= xgboost(data = data.matrix(dataset$trainNoLabels, rownames.force = NA),
label = trainLabels, nrounds = 1, objective = "binary:logistic", verbose = 0)
pred = predict(bst, data.matrix(dataset$testNoLabels, rownames.force = NA))
pred_hard = ifelse(pred < 0.50, 0, 1)
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Lendo a base pré processada
dataset = read.table('processed_train.csv', header=T, sep=';')
# Dividindo a base em k folds para validação cruzada
numOfFolds = 30
dataset = splitDataset(dataset, numOfFolds)
# Guardando resultados da área sob a curva ROC
aucResults = list()
for(i in 1:numOfFolds){
dt = dtTest(dataset[[i]])
aucResults$dt = c(aucResults$dt, dt[[1]])
boost = xgboostTest(dataset[[i]])
aucResults$boost = c(aucResults$boost, boost[[1]])
deep = deepTest(dataset[[i]], FALSE)
aucResults$deep = c(aucResults$deep, deep[[1]])
deep_balanced = deepTest(dataset[[i]], TRUE)
aucResults$deep_balanced = c(aucResults$deep_balanced, deep_balanced[[1]])
}
pacman::p_load('h2o')
h2o.init(nthreads=-1)
library('ROSE')
library('rpart')
library('xgboost')
library('caret')
path = 'C:/Projects/case_celpe/celpe_case'
setwd(path)
source('utils.r')
# Decision Tree
dtTest = function(dataset){
dt_model = rpart(dataset$trainLabels ~ ., data = dataset$trainNoLabels)
pred = predict(dt_model, newdata = dataset$testNoLabels)
pred_hard = ifelse(pred < 0.50, 0, 1)
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Balanced Deep Learning
deepTest = function(dataset, balanced){
# Transformando alvo em fator, para o modelo entender que a variável é binária e não numérica
dataset$train[, 'TARGET'] = as.factor(dataset$train[, 'TARGET'])
dataset$test[, 'TARGET'] = as.factor(dataset$test[, 'TARGET'])
dl_model = h2o.deeplearning(y='TARGET', training_frame=as.h2o(dataset$train), validation_frame=as.h2o(dataset$test),
hidden=c(50, 50), epochs=1, activation='Tanh',
score_training_samples=0,
score_validation_samples=0,
balance_classes=balanced,
quiet_mode = TRUE)
pred = h2o.predict(dl_model, as.h2o(dataset$test))
pred = as.data.frame(pred)
pred_hard = pred[,1]
pred_prob = pred[,3]
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred_prob, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Extreme Gradient Boosting
xgboostTest = function(dataset){
# Alvo precisa ser 0 e 1 e numérico
trainLabels = as.numeric(dataset$trainLabels)
trainLabels[which(trainLabels == 2)] = 0
bst= xgboost(data = data.matrix(dataset$trainNoLabels, rownames.force = NA),
label = trainLabels, nrounds = 1, objective = "binary:logistic", verbose = 0)
pred = predict(bst, data.matrix(dataset$testNoLabels, rownames.force = NA))
pred_hard = ifelse(pred < 0.50, 0, 1)
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Lendo a base pré processada
dataset = read.table('processed_train.csv', header=T, sep=';')
# Dividindo a base em k folds para validação cruzada
numOfFolds = 10
dataset = splitDataset(dataset, numOfFolds)
# Guardando resultados da área sob a curva ROC
aucResults = list()
for(i in 1:numOfFolds){
dt = dtTest(dataset[[i]])
aucResults$dt = c(aucResults$dt, dt[[1]])
boost = xgboostTest(dataset[[i]])
aucResults$boost = c(aucResults$boost, boost[[1]])
deep = deepTest(dataset[[i]], FALSE)
aucResults$deep = c(aucResults$deep, deep[[1]])
deep_balanced = deepTest(dataset[[i]], TRUE)
aucResults$deep_balanced = c(aucResults$deep_balanced, deep_balanced[[1]])
}
saveRDS(aucResults, "aucResults.rds")
#aucResults = readRDS("aucResults.rds")
# Analisando os resultados
summary(aucResults$boost)
summary(aucResults$dt)
summary(aucResults$deep)
# Aplicando testes de hipótese para garantir que os resultados diferem
# Rejeita a hipótese que os resultados são iguais
test0 = friedman.test(cbind( aucResults$dt, aucResults$boost, aucResults$deep))
# Não rejeita a hipótese de que um método é melhor que o outro
test1 = wilcox.test(aucResults$boost, aucResults$dt, paired = TRUE, alternative = "less", conf.level = 0.95)
test2 = wilcox.test(aucResults$boost, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
test3 = wilcox.test(aucResults$dt, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
test0 = friedman.test(cbind( aucResults$deep_balanced aucResults$deep))
test0 = friedman.test(cbind( aucResults$deep_balanced, aucResults$deep))
test0
summary(aucResults$deep)
summary(aucResults$deep_balanced)
test_equal = friedman.test(cbind( aucResults$dt, aucResults$boost, aucResults$deep, aucResults$deep_balanced))
test_equal
wilcox.test(aucResults$deep_balanced, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
wilcox.test(aucResults$boost, aucResults$dt, paired = TRUE, alternative = "less", conf.level = 0.95)
summary(aucResults$dt)
summary(aucResults$boost)
wilcox.test(aucResults$dt, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
wilcox.test(aucResults$boost, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
pacman::p_load('h2o')
h2o.init(nthreads=-1)
library('ROSE')
library('rpart')
library('xgboost')
library('caret')
path = 'C:/Projects/case_celpe/celpe_case'
setwd(path)
source('utils.r')
# Decision Tree
dtTest = function(dataset){
dt_model = rpart(dataset$trainLabels ~ ., data = dataset$trainNoLabels)
pred = predict(dt_model, newdata = dataset$testNoLabels)
pred_hard = ifelse(pred < 0.50, 0, 1)
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Balanced Deep Learning
deepTest = function(dataset, balanced){
# Transformando alvo em fator, para o modelo entender que a variável é binária e não numérica
dataset$train[, 'TARGET'] = as.factor(dataset$train[, 'TARGET'])
dataset$test[, 'TARGET'] = as.factor(dataset$test[, 'TARGET'])
dl_model = h2o.deeplearning(y='TARGET', training_frame=as.h2o(dataset$train), validation_frame=as.h2o(dataset$test),
hidden=c(50, 50), epochs=1, activation='Tanh',
score_training_samples=0,
score_validation_samples=0,
balance_classes=balanced,
quiet_mode = TRUE)
pred = h2o.predict(dl_model, as.h2o(dataset$test))
pred = as.data.frame(pred)
pred_hard = pred[,1]
pred_prob = pred[,3]
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred_prob, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Extreme Gradient Boosting
xgboostTest = function(dataset){
# Alvo precisa ser 0 e 1 e numérico
trainLabels = as.numeric(dataset$trainLabels)
trainLabels[which(trainLabels == 2)] = 0
bst= xgboost(data = data.matrix(dataset$trainNoLabels, rownames.force = NA),
label = trainLabels, nrounds = 10, objective = "binary:logistic", verbose = 0)
pred = predict(bst, data.matrix(dataset$testNoLabels, rownames.force = NA))
pred_hard = ifelse(pred < 0.50, 0, 1)
cfMatrix = confusionMatrix(as.factor(pred_hard), as.factor(dataset$testLabels))
auc = roc.curve(dataset$testLabels, pred, plotit = F)$auc
return (list(auc, cfMatrix))
}
# Lendo a base pré processada
dataset = read.table('processed_train.csv', header=T, sep=';')
# Dividindo a base em k folds para validação cruzada
numOfFolds = 10
dataset = splitDataset(dataset, numOfFolds)
# Guardando resultados da área sob a curva ROC
aucResults = list()
for(i in 1:numOfFolds){
dt = dtTest(dataset[[i]])
aucResults$dt = c(aucResults$dt, dt[[1]])
boost = xgboostTest(dataset[[i]])
aucResults$boost = c(aucResults$boost, boost[[1]])
deep = deepTest(dataset[[i]], FALSE)
aucResults$deep = c(aucResults$deep, deep[[1]])
deep_balanced = deepTest(dataset[[i]], TRUE)
aucResults$deep_balanced = c(aucResults$deep_balanced, deep_balanced[[1]])
}
saveRDS(aucResults, "aucResults.rds")
#aucResults = readRDS("aucResults.rds")
# Analisando os resultados
summary(aucResults$boost)
summary(aucResults$dt)
summary(aucResults$deep)
# Aplicando testes de hipótese para garantir que os resultados diferem
# Rejeita a hipótese que os resultados são iguais
testModels = friedman.test(cbind( aucResults$dt, aucResults$boost, aucResults$deep, aucResults$deep_balanced))
# Não rejeita a hipótese de que um método é melhor que o outro
testDeep = wilcox.test(aucResults$deep_balanced, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
testBoostDt = wilcox.test(aucResults$boost, aucResults$dt, paired = TRUE, alternative = "less", conf.level = 0.95)
testBoostDeep = wilcox.test(aucResults$boost, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
testDtDeep = wilcox.test(aucResults$dt, aucResults$deep, paired = TRUE, alternative = "less", conf.level = 0.95)
summary(aucResults$boost)
summary(aucResults$dt)
summary(aucResults$deep)
summary(aucResults$deep_balanced)
dt_model = rpart(dataset[[1]]$trainLabels ~ ., data = dataset[[1]]$trainNoLabels)
dt_model
summary(dt_model)
print(dt_model)
rpart.plot(dt_model)
library('rpart.plot')
install.packages('rpart.plot')
library('rpart.plot')
rpart.plot(dt_model)
print(dt_model)
